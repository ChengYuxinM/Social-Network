{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite import"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\r\n",
        "import pandas as pd\r\n",
        "from collections import OrderedDict\r\n",
        "import json\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653630897706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ignite.engine import *\r\n",
        "from ignite.handlers import *\r\n",
        "from ignite.metrics import *\r\n",
        "from ignite.utils import *\r\n",
        "from ignite.contrib.metrics.regression import *\r\n",
        "from ignite.contrib.metrics import *\r\n",
        "\r\n",
        "def eval_step(engine, batch):\r\n",
        "    return batch\r\n",
        "\r\n",
        "default_evaluator = Engine(eval_step)\r\n",
        "\r\n",
        "param_tensor = torch.zeros([1], requires_grad=True)\r\n",
        "default_optimizer = torch.optim.SGD([param_tensor], lr=0.1)\r\n",
        "\r\n",
        "def get_default_trainer():\r\n",
        "\r\n",
        "    def train_step(engine, batch):\r\n",
        "        return batch\r\n",
        "\r\n",
        "    return Engine(train_step)\r\n",
        "\r\n",
        "# create default model for doctests\r\n",
        "\r\n",
        "default_model = nn.Sequential(OrderedDict([\r\n",
        "    ('base', nn.Linear(4, 2)),\r\n",
        "    ('fc', nn.Linear(2, 1))\r\n",
        "]))\r\n",
        "\r\n",
        "manual_seed(666)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653630929293
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def load_obj(name ):\r\n",
        "    with open('./' + name + '.pkl', 'rb') as f:\r\n",
        "        return pickle.load(f)\r\n",
        "\r\n",
        "users = pickle.load(open(\"users_new.pkl\", 'rb'))\r\n",
        "reviews = pickle.load(open(\"reviews.pkl\", 'rb'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653631322901
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "PXn6vssRB36T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_1 = 0\r\n",
        "for item in users:\r\n",
        "    view_seq = [[0, 0, 0, 0, 0, 0, 0, 0, 0] for i in range(124)]\r\n",
        "    labels = 0\r\n",
        "    year = 2012\r\n",
        "    # print(item)\r\n",
        "    if len(users[item][\"reviews\"]) == 0:\r\n",
        "        continue\r\n",
        "    for review in users[item][\"reviews\"]:\r\n",
        "        view = reviews[review].copy()\r\n",
        "        # print(view[\"date\"])\r\n",
        "        view[\"date\"] = view[\"date\"].split('-')\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][0] += 1\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][1] += view[\"stars\"]\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][2] += view[\"funny\"]\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][3] += view[\"useful\"]\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][4] += view[\"cool\"]\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][5] += view[\"sentiment_score\"]['neg']\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][6] += view[\"sentiment_score\"]['neu']\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][7] += view[\"sentiment_score\"]['pos']\r\n",
        "        view_seq[(int(view[\"date\"][0]) - year) * 12 + int(view[\"date\"][1]) - 1][8] += view[\"sentiment_vader\"]\r\n",
        "    for i in range(len(view_seq)):\r\n",
        "        if view_seq[i][0] != 0:\r\n",
        "            view_seq[i][1] /= view_seq[i][0]\r\n",
        "            view_seq[i][2] /= view_seq[i][0]\r\n",
        "            view_seq[i][3] /= view_seq[i][0]\r\n",
        "            view_seq[i][4] /= view_seq[i][0]\r\n",
        "            view_seq[i][5] /= view_seq[i][0]\r\n",
        "            view_seq[i][6] /= view_seq[i][0]\r\n",
        "            view_seq[i][7] /= view_seq[i][0]\r\n",
        "            view_seq[i][8] /= view_seq[i][0]\r\n",
        "\r\n",
        "    sum_score = 0\r\n",
        "    for i in range(len(view_seq)-36, len(view_seq)):\r\n",
        "        sum_score += view_seq[i][0] \r\n",
        "    if sum_score > 0:       \r\n",
        "        labels = 1\r\n",
        "        label_1 += 1\r\n",
        "    users[item][\"view_seq\"] = view_seq\r\n",
        "    users[item][\"labels\"] = labels\r\n",
        "print(label_1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653631771187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in users:\r\n",
        "    label_churn = 0\r\n",
        "    if len(users[item]['friends_valid']) == 0:\r\n",
        "        users[item]['friend_churn'] = 1\r\n",
        "        continue\r\n",
        "    for f in users[item]['friends_valid']:\r\n",
        "        if users[f]['labels'] == 0:\r\n",
        "            label_churn += 1\r\n",
        "    users[item]['friend_churn'] = label_churn / len(users[item]['friends_valid'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653632069207
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in users:\n",
        "  print(users[item][\"view_seq\"][:])\n",
        "  for i in range(len(users[item]['view_seq'])):\n",
        "    if users[item]['view_seq'][i][0] > 0:\n",
        "      print(i)\n",
        "  print(users[item]['labels'])\n",
        "  #for i in range(len(users[item]['labels'])):\n",
        "    # if users[item]['labels'][i] > 0:\n",
        "    #   print(i)\n",
        "  print(users[item]['since'])\n",
        "  break"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 654,
          "status": "ok",
          "timestamp": 1652520537410,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          },
          "user_tz": -480
        },
        "id": "Z5U841frgYUU",
        "outputId": "8486ae22-5224-4db5-b52b-2f65322e8351",
        "gather": {
          "logged": 1653632080724
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for item in users: \n",
        "  features = np.array(users[item][\"view_seq\"])\n",
        "  features = features[:88].sum(axis = 0)\n",
        "  users[item]['view_features'] = features\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "83x-xqTIUEY-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652520609554,
          "user_tz": -480,
          "elapsed": 17695,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "gather": {
          "logged": 1653632520452
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def FeaturesProcess(users, reviews):\r\n",
        "  seq = {}\r\n",
        "  for item in users:\r\n",
        "    user_features = np.array([])\r\n",
        "    user_features = np.append(user_features, users[item]['review_count'])\r\n",
        "    user_features = np.append(user_features, users[item]['fans'])\r\n",
        "    user_features = np.append(user_features, len(users[item]['friends']))\r\n",
        "    user_features = np.append(user_features, users[item]['friend_churn'])\r\n",
        "\r\n",
        "\r\n",
        "    user_features = np.append(user_features, users[item]['comments'])\r\n",
        "    user_features = np.append(user_features, users[item]['compliment'])\r\n",
        "    user_features = np.append(user_features, users[item]['stars'])\r\n",
        "    \r\n",
        "\r\n",
        "    user_features = np.hstack((user_features,users[item]['view_features']))\r\n",
        "    user_features = np.hstack((user_features,users[item]['labels']))\r\n",
        "    seq[item] = user_features\r\n",
        "  return seq\r\n",
        "\r\n",
        "features_seq = FeaturesProcess(users, reviews)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653632649068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in features_seq:\n",
        "  print(features_seq[item])\n",
        "  print(features_seq[item].shape)\n",
        "  break\n",
        "  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IeTB91nGxHS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652520613309,
          "user_tz": -480,
          "elapsed": 4,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "outputId": "9e9bed0b-8e21-45c6-8260-a3bfdaae1226",
        "gather": {
          "logged": 1653632657920
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR Mode"
      ],
      "metadata": {
        "id": "9-xg-Cpi_PyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def features_org(seq):\n",
        "\n",
        "    train_Xs = []\n",
        "    train_Ys = []\n",
        "    train_Us = []\n",
        "    test_Xs = []\n",
        "    test_Ys = []\n",
        "    test_Us = []\n",
        "    train_len0 = 0\n",
        "    train_len1 = 0\n",
        "    test_len0 = 0\n",
        "    test_len1 = 0\n",
        "    count = 0\n",
        "\n",
        "    review_count = 0\n",
        "\n",
        "    for item in seq:\n",
        "        features = seq[item]\n",
        "        # if random.random() > 0.1:\n",
        "        #     continue\n",
        "\n",
        "        if random.random() < 0.9:\n",
        "            if features[-1] == 0 and train_len0 > 1.1 * train_len1:\n",
        "                continue\n",
        "\n",
        "            review_count += users[item]['review_count_valid']\n",
        "\n",
        "            if features[-1] == 1:\n",
        "                train_len1 += 1\n",
        "                count+=1\n",
        "            else:\n",
        "                train_len0 += 1\n",
        "            train_Us.append(features[:13])\n",
        "            train_Xs.append(features[13:-1])\n",
        "            train_Ys.append([features[-1]])\n",
        "            \n",
        "        else:\n",
        "            if features[-1] == 0 and test_len0 > 1.1 * test_len1:\n",
        "                continue\n",
        "\n",
        "            review_count += users[item]['review_count_valid']\n",
        "\n",
        "            if features[-1] == 1:\n",
        "                test_len1 += 1\n",
        "                count+=1 \n",
        "            else:\n",
        "                test_len0 += 1\n",
        "            test_Us.append(features[:13])\n",
        "            test_Xs.append(features[13:-1])\n",
        "            test_Ys.append([features[-1]])\n",
        "\n",
        "\n",
        "    print(count)\n",
        "    print(review_count)       \n",
        "    return (torch.tensor(train_Us), torch.tensor(train_Xs), torch.tensor(train_Ys), torch.tensor(test_Us), torch.tensor(test_Xs), torch.tensor(test_Ys))\n",
        "(train_Us, train_Xs, train_Ys, test_Us, test_Xs, test_Ys) = features_org(features_seq)\n",
        "\n",
        "print(len(test_Us))\n",
        "print(len(test_Xs))\n",
        "print(len(test_Ys))\n",
        "print(len(train_Us))\n",
        "print(len(train_Xs))\n",
        "print(len(train_Ys))\n",
        "print(test_Xs.shape)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJJmWmYOX4Jb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652523486987,
          "user_tz": -480,
          "elapsed": 766,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "outputId": "aa062f2a-a977-4d4b-ca77-7eab9994c89e",
        "gather": {
          "logged": 1653632679270
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_Us))\r\n",
        "print(len(test_Xs))\r\n",
        "print(len(test_Ys))\r\n",
        "print(len(train_Us))\r\n",
        "print(len(train_Xs))\r\n",
        "print(len(train_Ys))\r\n",
        "print(test_Us.shape)\r\n",
        "\r\n",
        "s = 0\r\n",
        "for i in train_Ys:\r\n",
        "    if i == 1:\r\n",
        "        s+=1\r\n",
        "print(s)\r\n",
        "\r\n",
        "s = 0\r\n",
        "for i in test_Ys:\r\n",
        "    if i == 1:\r\n",
        "        s+=1\r\n",
        "print(s)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653632746054
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(reviews))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1653632746264
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Seq2SeqDataset(Dataset):   \n",
        "    def __init__(self, U, X, Y):\n",
        "        super().__init__()\n",
        "        \n",
        "        # check inputs\n",
        "        assert len(X)==len(Y) and len(X) == len(U) and len(X)>0\n",
        "        \n",
        "        self.Us = U\n",
        "        self.Xs = X\n",
        "        self.Ys = Y\n",
        "\n",
        "        self.num_example = len(U)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.Us[idx], self.Xs[idx], self.Ys[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_example\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "q8y6VGzjCii3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652523489761,
          "user_tz": -480,
          "elapsed": 2,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "gather": {
          "logged": 1653632746354
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LR(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, user_dim, view_dim, pre, mode = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.pre = pre\n",
        "\n",
        "        \n",
        "        self.norm1 = nn.BatchNorm1d(view_dim)\n",
        "        self.norm2 = nn.BatchNorm1d(user_dim)\n",
        "        self.acti = nn.ELU()\n",
        "        self.pre = nn.Sigmoid()\n",
        "        self.fine = nn.Softmax(dim = 1)\n",
        "        self.drop = nn.Dropout2d(0.2)\n",
        "\n",
        "        self.linear1 = nn.Linear(input_dim, 32)\n",
        "        self.linear2 = nn.Linear(32, 64)\n",
        "        self.linear3 = nn.Linear(64, 48)\n",
        "        self.linear4 = nn.Linear(48, 16)\n",
        "        self.linear5 = nn.Linear(16, 8)\n",
        "\n",
        "        self.linear6 = nn.Linear(8, 2)\n",
        "        self.linear7 = nn.Linear(8, 1)\n",
        "\n",
        "        self.linear8 = nn.Linear(64, 128)\n",
        "        self.linear9 = nn.Linear(128, 256)\n",
        "        self.linear10 = nn.Linear(256, 256)\n",
        "        self.linear11 = nn.Linear(256, 192)\n",
        "        self.linear12 = nn.Linear(192, 192)\n",
        "        self.linear13 = nn.Linear(192, 128)\n",
        "        self.linear14 = nn.Linear(128, 96)\n",
        "        self.linear15 = nn.Linear(96, 64)\n",
        "        self.linear16 = nn.Linear(64, 64)\n",
        "\n",
        "\n",
        "    def forward(self, X, U):\n",
        "        X = self.norm1(X)\n",
        "        U = self.norm2(U)\n",
        "        \n",
        "        if self.mode  == 0:\n",
        "            input = torch.cat([X,U],axis = 1)\n",
        "        elif self.mode  == 1:\n",
        "            input = X\n",
        "        else:\n",
        "            input = U\n",
        "\n",
        "        hidden = self.linear1(input)\n",
        "        hidden = self.linear2(hidden)\n",
        "\n",
        "        hidden = self.acti(hidden)\n",
        "        hidden = self.linear8(hidden)\n",
        "        hidden = self.drop(hidden)\n",
        "        hidden = self.linear9(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "        hidden = self.linear10(hidden)\n",
        "        hidden = self.drop(hidden)\n",
        "        hidden = self.linear11(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "        hidden = self.linear12(hidden)\n",
        "        hidden = self.drop(hidden)\n",
        "        hidden = self.linear13(hidden)\n",
        "        hidden = self.drop(hidden)\n",
        "        hidden = self.linear14(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "        hidden = self.linear15(hidden)\n",
        "        hidden = self.drop(hidden)\n",
        "        hidden = self.linear16(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "\n",
        "        hidden = self.linear3(hidden)\n",
        "        hidden = self.linear4(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "        hidden = self.linear5(hidden)\n",
        "        hidden = self.acti(hidden)\n",
        "\n",
        "        if True:\n",
        "            y_hat = self.linear7(hidden)\n",
        "            y_hat = self.pre(y_hat)\n",
        "        else:\n",
        "            y_hat = self.linear6(hidden)\n",
        "            y_hat = self.fine(y_hat)\n",
        "\n",
        "        return y_hat\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qEH6zzRuXsPc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652523489761,
          "user_tz": -480,
          "elapsed": 2,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "gather": {
          "logged": 1653632753222
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_count_1(pred, true):\n",
        "    count = 0\n",
        "    for i in range(len(pred)):\n",
        "        if (pred[i][0] > pred[i][1] and true[i][0] == 0) or (pred[i][1] >= pred[i][0] and true[i][0] == 1):\n",
        "            count += 1\n",
        "    return count / len(true)\n",
        "\n",
        "def loss_count(pred, true):\n",
        "    count = 0\n",
        "    for i in range(len(pred)):  \n",
        "        if (pred[i][0] >= 0.5 and true[i][0] == 1) or (pred[i][0] < 0.5 and true[i][0] == 0):\n",
        "            count += 1\n",
        "    return count / len(true)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EAHq36RUjfJ4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652523491831,
          "user_tz": -480,
          "elapsed": 3,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "gather": {
          "logged": 1653632754496
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_dim = 22\n",
        "output_dim = 2\n",
        "batch_size = 64\n",
        "user_dim = 13\n",
        "view_dim = 9\n",
        "learning_rate = 0.05\n",
        "LRpre = True\n",
        "\n",
        "loss_fn = nn.MSELoss()# nn.BCELoss()\n",
        "loss_fn_new = nn.CrossEntropyLoss()\n",
        "\n",
        "train_Xs1 = train_Xs[:]\n",
        "train_Us1 = train_Us[:]\n",
        "train_Ys1 = train_Ys[:]\n",
        "\n",
        "test_Xs1 = test_Xs[:]\n",
        "test_Us1 = test_Us[:]\n",
        "test_Ys1 = test_Ys[:]\n",
        "\n",
        "train_dataset = Seq2SeqDataset(train_Us1, train_Xs1, train_Ys1)\n",
        "test_dataset = Seq2SeqDataset(test_Us1, test_Xs1, test_Ys1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = LR(input_dim, output_dim, user_dim, view_dim, LRpre, 0)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "model.double()\n",
        "\n",
        "\n",
        "roc_auc = ROC_AUC()\n",
        "roc_auc.attach(default_evaluator, 'roc_auc')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "a9tuUWAYZIfn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652523498337,
          "user_tz": -480,
          "elapsed": 722,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "gather": {
          "logged": 1653632823457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "# model = model.double().cuda()\n",
        "\n",
        "best_score = 0\n",
        "metric = []\n",
        "\n",
        "epoch = 10\n",
        "for epoch in range(epoch):\n",
        "    # training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        # forward pass\n",
        "        U, X, Y = batch\n",
        "        U, X, Y = U, X, Y\n",
        "        y_hat = model(X, U)\n",
        "\n",
        "        if LRpre:\n",
        "            loss = loss_fn(y_hat.double(), Y.double())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss.detach())\n",
        "        else:\n",
        "            loss = loss_fn_new(y_hat.double().squeeze(0), torch.cat([Y[i] for i in range(len(Y))], 0).long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss.detach())\n",
        "\n",
        "    # calculate epoch loss and MAPE\n",
        "    train_loss = torch.mean(torch.tensor(losses)).item()\n",
        "\n",
        "    # evaluation loop\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    loss_num = []\n",
        "    auc_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            # forward pass\n",
        "            U, X, Y = batch\n",
        "            U, X, Y = U, X, Y\n",
        "            y_hat = model(X, U)\n",
        "            if LRpre:\n",
        "                loss = loss_fn(y_hat.double(), Y.double())\n",
        "                loss_num.append(loss_count(y_hat, Y))\n",
        "                losses.append(loss.detach())\n",
        "                #state = default_evaluator.run([[model(X, U), Y]])\n",
        "                #print(state.metrics['roc_auc'])\n",
        "                auc_list.append(default_evaluator.run([[y_hat, Y]]).metrics['roc_auc'])\n",
        "            else:\n",
        "                loss = loss_fn_new(y_hat.double().squeeze(0), torch.cat([Y[i] for i in range(len(Y))], 0).long())\n",
        "                loss_num.append(loss_count_1(y_hat.double().squeeze(0), Y.double()))\n",
        "                losses.append(loss.detach())\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "        # calculate evaluateion loss and MAPE for current epoch\n",
        "        eval_loss = torch.mean(torch.tensor(losses)).item()\n",
        "        count_loss = torch.mean(torch.tensor(loss_num)).item()\n",
        "        if LRpre:\n",
        "            auc_loss = torch.mean(torch.tensor(auc_list)).item()\n",
        "        else:\n",
        "            auc_loss = 0\n",
        "\n",
        "        if count_loss > best_score:\n",
        "            best_score = count_loss\n",
        "            best_ckpt = model.state_dict()\n",
        "    # record training curves and metrics\n",
        "    metrics = {\n",
        "        'epoch'   : epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'eval_loss' : eval_loss,\n",
        "        'pred' : count_loss,\n",
        "        'auc' : auc_loss\n",
        "    }\n",
        "    history.append(metrics)\n",
        "    metric.append(metrics)\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "print(best_score)\n",
        "torch.save({\n",
        "    'model_state_dict': best_ckpt,\n",
        "}, \"./LR_checkpoint.ckpt\")\n",
        "\n",
        "\n",
        "jsObj = json.dumps(metric)\n",
        "fileObject = open('jsonFileLR.json', 'w')\n",
        "fileObject.write(jsObj)\n",
        "fileObject.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "Q0bAi8NwZIZu",
        "executionInfo": {
          "status": "error",
          "timestamp": 1652523530742,
          "user_tz": -480,
          "elapsed": 30192,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "outputId": "f9c7d3a6-a41a-4065-986d-2135153448fc",
        "gather": {
          "logged": 1653632954947
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semi-RNN Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vTTRrBNrkC-O",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652519576810,
          "user_tz": -480,
          "elapsed": 676,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for item in users:\r\n",
        "#   view_seq = []\r\n",
        "#   for i in range(88):\r\n",
        "#     if sum(users[item][\"view_seq\"][i]) != 0:\r\n",
        "#         users[item][\"view_seq\"][i].append(i)\r\n",
        "#         view_seq.append(users[item][\"view_seq\"][i])\r\n",
        "#   users[item][\"view_seq_valid\"] = view_seq"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vf5f3m4vZIUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\r\n",
        "def features_org(users, features_seq):\r\n",
        "\r\n",
        "    train_Xs = []\r\n",
        "    train_Ys = []\r\n",
        "    train_Us = []\r\n",
        "    test_Xs = []\r\n",
        "    test_Ys = []\r\n",
        "    test_Us = []\r\n",
        "    train_len0 = 0\r\n",
        "    train_len1 = 0\r\n",
        "    test_len0 = 0\r\n",
        "    test_len1 = 0\r\n",
        "    count = 0\r\n",
        "\r\n",
        "    review_count = 0\r\n",
        "\r\n",
        "    for item in users:\r\n",
        "        label = users[item]['labels']\r\n",
        "        # if random.random() > 0.1:\r\n",
        "        #     continue\r\n",
        "\r\n",
        "        if random.random() < 0.9:\r\n",
        "            if label == 0 and train_len0 > 1.1 * train_len1:\r\n",
        "                continue\r\n",
        "\r\n",
        "            review_count += users[item]['review_count_valid']\r\n",
        "\r\n",
        "            if label == 1:\r\n",
        "                train_len1 += 1\r\n",
        "                count+=1\r\n",
        "            else:\r\n",
        "                train_len0 += 1\r\n",
        "\r\n",
        "            train_Us.append(features_seq[item][:13])\r\n",
        "            train_Xs.append(users[item]['view_seq'][:88])\r\n",
        "            train_Ys.append(label)\r\n",
        "            \r\n",
        "        else:\r\n",
        "            if label == 0 and test_len0 > 1.1 * test_len1:\r\n",
        "                continue\r\n",
        "\r\n",
        "            review_count += users[item]['review_count_valid']\r\n",
        "\r\n",
        "            if label == 1:\r\n",
        "                test_len1 += 1\r\n",
        "                count+=1 \r\n",
        "            else:\r\n",
        "                test_len0 += 1\r\n",
        "\r\n",
        "            test_Us.append(features_seq[item][:13])\r\n",
        "            test_Xs.append(users[item]['view_seq'][:88])\r\n",
        "            test_Ys.append(label)\r\n",
        "\r\n",
        "\r\n",
        "    print(count)\r\n",
        "    print(review_count)       \r\n",
        "    return (torch.tensor(train_Us), torch.tensor(train_Xs), torch.tensor(train_Ys), torch.tensor(test_Us), torch.tensor(test_Xs), torch.tensor(test_Ys))\r\n",
        "(train_Us, train_Xs, train_Ys, test_Us, test_Xs, test_Ys) = features_org(users, features_seq)\r\n",
        "\r\n",
        "print(len(test_Us))\r\n",
        "print(len(test_Xs))\r\n",
        "print(len(test_Ys))\r\n",
        "print(len(train_Us))\r\n",
        "print(len(train_Xs))\r\n",
        "print(len(train_Ys))\r\n",
        "print(test_Xs.shape)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "B2_HdkYBZIRe",
        "gather": {
          "logged": 1653633621943
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, enc_input_dim, output_dim, user_dim, hidden_dim, layers, bid, user):\n",
        "        super().__init__()\n",
        "        # encoder\n",
        "        self.norm1 = nn.BatchNorm1d(enc_input_dim)\n",
        "        self.norm2 = nn.BatchNorm1d(user_dim)\n",
        "        self.norm3 = nn.BatchNorm1d(hidden_dim)\n",
        "        \n",
        "        self.enc_rnn = nn.GRU(\n",
        "            input_size=enc_input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers= layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bid\n",
        "        )\n",
        "\n",
        "        self.bid = bid\n",
        "        self.user = user\n",
        "        if bid:\n",
        "            linear_dim = hidden_dim * 2 * layers + user\n",
        "            self.norm3 = nn.BatchNorm1d(hidden_dim * 2 * layers)\n",
        "        else:\n",
        "            linear_dim = hidden_dim * layers + user\n",
        "            self.norm3 = nn.BatchNorm1d(hidden_dim * layers)\n",
        "\n",
        "        # user LR\n",
        "        self.linear7 = nn.Linear(user_dim, 32)\n",
        "        self.linear8 = nn.Linear(32, 64)\n",
        "        self.linear9 = nn.Linear(64, 128)\n",
        "        self.linear10 = nn.Linear(128, 48)\n",
        "        self.linear11 = nn.Linear(48, 32)\n",
        "        self.linear12 = nn.Linear(32, user_dim)\n",
        "\n",
        "\n",
        "        # linear predictor\n",
        "        self.linear1 = nn.Linear(linear_dim, 512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.linear3 = nn.Linear(256, 128)\n",
        "        self.linear4 = nn.Linear(128, 64)\n",
        "        self.linear5 = nn.Linear(64, 32)\n",
        "        self.linear6 = nn.Linear(32, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "        self.acti = nn.ELU()\n",
        "        self.drop = nn.Dropout2d(0.2)\n",
        "        self.pre = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, X, U):\n",
        "        X = self.norm1(X.transpose(1, 2)).transpose(1, 2)\n",
        "        U = self.norm2(U)\n",
        "        output, hidden = self.enc_rnn(X)  # encoding\n",
        "        \n",
        "        U = self.linear12(self.linear11(self.linear10(self.acti(self.linear9(self.acti(self.linear8(self.acti(self.linear7(U)))))))))\n",
        "        \n",
        "        hidden = torch.cat([hidden[i] for i in range(len(hidden))], 1).unsqueeze(0)\n",
        "\n",
        "        hidden = self.norm3(hidden.squeeze(0)).unsqueeze(0)\n",
        "        \n",
        "        if self.user > 0:\n",
        "            hidden = torch.cat([hidden, U.unsqueeze(0)], 2)\n",
        "\n",
        "        # print(hidden)\n",
        "        y_hat = self.linear6(self.linear5(self.linear4(self.acti(self.linear3(self.acti(self.linear2(self.acti(self.linear1(hidden)))))))))  # predicting\n",
        "        y_hat = self.pre(y_hat)\n",
        "  \n",
        "        return y_hat"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "YD_RFwd4RzFG",
        "gather": {
          "logged": 1653640238895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_dim = 9\n",
        "output_dim = 1\n",
        "hidden_dim = 48\n",
        "batch_size = 32\n",
        "user_dim = 13\n",
        "learning_rate = 0.1\n",
        "loss_fn = nn.BCELoss()# nn.BCELoss()\n",
        "\n",
        "\n",
        "train_Xs1 = train_Xs[:]\n",
        "train_Us1 = train_Us[:]\n",
        "train_Ys1 = train_Ys[:]\n",
        "\n",
        "test_Xs1 = test_Xs[:]\n",
        "test_Us1 = test_Us[:]\n",
        "test_Ys1 = test_Ys[:]\n",
        "\n",
        "train_dataset = Seq2SeqDataset(train_Us1, train_Xs1, train_Ys1)\n",
        "test_dataset = Seq2SeqDataset(test_Us1, test_Xs1, test_Ys1)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_loader  = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "et5ML5hSSMHb",
        "gather": {
          "logged": 1653640240238
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "newmodel = RNNmodel(input_dim, output_dim, user_dim, hidden_dim, 10, True, len(train_Us[0]))\n",
        "newmodel = newmodel.double().cuda()\n",
        "optimizer = torch.optim.Adam(newmodel.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JZeX0Pr1Gw8p",
        "gather": {
          "logged": 1653640240319
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Xs1.unsqueeze(0).shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbUGbEnlCGHo",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1652501550645,
          "user_tz": -480,
          "elapsed": 3,
          "user": {
            "displayName": "Yuxin Cheng",
            "userId": "07688075878303638427"
          }
        },
        "outputId": "494ba622-26d8-436c-e9f3-5b36e017e535",
        "gather": {
          "logged": 1653640240450
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_count_1(pred, true):\n",
        "    count = 0\n",
        "    for i in range(len(pred)):\n",
        "        if (pred[i][0] > pred[i][1] and true[i][0] == 0) or (pred[i][1] >= pred[i][0] and true[i][0] == 1):\n",
        "            count += 1\n",
        "    return count / len(true)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "yr2a0t1YE0rS",
        "gather": {
          "logged": 1653640241605
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "# model = model.double().cuda()\n",
        "\n",
        "best_score = 0\n",
        "metric = []\n",
        "epoch = 2\n",
        "for epoch in range(epoch):\n",
        "    # training loop\n",
        "    newmodel.train()\n",
        "    losses = []\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        # forward pass\n",
        "        U, X, Y = batch\n",
        "        U, X, Y = U.double().cuda(), X.double().cuda(), Y.double().cuda()\n",
        "        y_hat = newmodel(X, U)\n",
        "\n",
        "        # loss = loss_fn(y_hat.double().squeeze(0), Y.double())\n",
        "        loss = loss_fn(y_hat.double().squeeze(0), Y.reshape(-1,1).double())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        losses.append(loss.detach())\n",
        "\n",
        "    # calculate epoch loss and MAPE\n",
        "    train_loss = torch.mean(torch.tensor(losses)).item()\n",
        "\n",
        "    # evaluation loop\n",
        "    newmodel.eval()\n",
        "    losses = []\n",
        "    loss_num = []\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            # forward pass\n",
        "            U, X, Y = batch\n",
        "            U, X, Y = U.double().cuda(), X.double().cuda(), Y.double().cuda()\n",
        "            y_hat = newmodel(X, U)\n",
        "\n",
        "            loss = loss_fn(y_hat.double().squeeze(0), Y.reshape(-1,1).double())\n",
        "\n",
        "\n",
        "            loss_num.append(loss_count(y_hat.double().squeeze(0), Y.reshape(-1,1).double()))\n",
        "            losses.append(loss.detach())\n",
        "\n",
        "\n",
        "\n",
        "        # calculate evaluateion loss and MAPE for current epoch\n",
        "        eval_loss = torch.mean(torch.tensor(losses)).item()\n",
        "        count_loss = torch.mean(torch.tensor(loss_num)).item()\n",
        "        \n",
        "        if count_loss > best_score:\n",
        "            best_score = count_loss\n",
        "            best_ckpt = newmodel.state_dict()\n",
        "    # record training curves and metrics\n",
        "    metrics = {\n",
        "        'epoch'   : epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'eval_loss' : eval_loss,\n",
        "        'pred' : count_loss\n",
        "    }\n",
        "    history.append(metrics)\n",
        "    metric.append(metrics)\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "print(best_score)\n",
        "torch.save({\n",
        "    'model_state_dict': best_ckpt,\n",
        "}, \"./checkpoint.ckpt\")\n",
        "\n",
        "\n",
        "jsObj = json.dumps(item)\n",
        "fileObject = open('jsonFile.json', 'w')\n",
        "fileObject.write(jsObj)\n",
        "fileObject.close()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "W9NhXZQNiz4D",
        "gather": {
          "logged": 1653640484276
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "new_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIYM8GjZpHw1IrqChB5ocK"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}